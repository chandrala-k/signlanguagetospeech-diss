{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Advancing Real Time Sign Language to Speech using Transformers\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LdhDQMZ0Q5ax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Notebook contains the code for translation of Sign Language to Speech Using Transformers. The following code blocks include all necessary steps to process the data, build the model, and evaluate its performance.\n"
      ],
      "metadata": {
        "id": "mlvKv3UdTmzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount the Google Drive to the Colab environment\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "7J2fBFzTWzW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Libraries installed and used for the code"
      ],
      "metadata": {
        "id": "n9Ux9x3ku6Ct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python mediapipe numpy matplotlib\n",
        "!pip install mediapipe\n",
        "!pip install tensorflow numpy pandas\n",
        "!pip install gtts\n",
        "!pip install rouge_score\n",
        "!pip install googletrans==4.0.0-rc1\n",
        "!pip install jiwer"
      ],
      "metadata": {
        "id": "EdGZyWPlXM63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code sets up the base directory, and specific file paths for videos and annotations, and creates an output directory to store processed data."
      ],
      "metadata": {
        "id": "s705WOWebpTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the base directory within Google Drive where the dataset is located.\n",
        "DRIVE_BASE = '/content/drive/MyDrive/sign_language_dataset'\n",
        "\n",
        "# Specify the directory containing the video files.\n",
        "VIDEO_DIR = os.path.join(DRIVE_BASE, 'videos')\n",
        "\n",
        "# Define paths to the annotation files, which include glosses, corpus details, frame details, and word details.\n",
        "ANNOTATIONS_GLOSSES_PATH = os.path.join(DRIVE_BASE, 'corpus_csv_files', 'ISL Corpus sign glosses.csv')\n",
        "ANNOTATIONS_CORPUS_DETAILS_PATH = os.path.join(DRIVE_BASE, 'corpus_csv_files', 'ISL_CSLRT_Corpus details.xlsx')\n",
        "ANNOTATIONS_FRAME_DETAILS_PATH = os.path.join(DRIVE_BASE, 'corpus_csv_files', 'ISL_CSLRT_Corpus_frame_details.xlsx')\n",
        "ANNOTATIONS_WORD_DETAILS_PATH = os.path.join(DRIVE_BASE, 'corpus_csv_files', 'ISL_CSLRT_Corpus_word_details.xlsx')\n",
        "\n",
        "# Create an output directory for storing processed data, ensuring the directory exists.\n",
        "OUTPUT_DIR = os.path.join(DRIVE_BASE, 'processed_data')\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "F7PpUvmabzrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the glosses data from a CSV file into a pandas DataFrame.\n",
        "glosses_df = pd.read_csv(ANNOTATIONS_GLOSSES_PATH)\n",
        "\n",
        "# Load the corpus details from an Excel file into a pandas DataFrame.\n",
        "corpus_details_df = pd.read_excel(ANNOTATIONS_CORPUS_DETAILS_PATH)\n",
        "\n",
        "# Load the frame details from an Excel file into a pandas DataFrame.\n",
        "frame_details_df = pd.read_excel(ANNOTATIONS_FRAME_DETAILS_PATH)\n",
        "\n",
        "# Load the word details from an Excel file into a pandas DataFrame.\n",
        "word_details_df = pd.read_excel(ANNOTATIONS_WORD_DETAILS_PATH)\n"
      ],
      "metadata": {
        "id": "OU0mW-91wVRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The code below includes functions for converting sentence labels to filename-friendly formats, extracting frames at a specified rate from videos, and processing all video files in a directory.\n"
      ],
      "metadata": {
        "id": "vt2pCgvcwj02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "\n",
        "# Helper function to convert a sentence into a format suitable for filenames.\n",
        "# This function replaces spaces with underscores and removes punctuation marks.\n",
        "def sentence_to_filename(sentence):\n",
        "    return sentence.lower().replace(' ', '_').replace('?', '').replace('.', '').replace(',', '').replace('!', '')\n",
        "\n",
        "# Function to extract frames from a video at a specified frame rate.\n",
        "# The frames are stored in a list and returned.\n",
        "def extract_frames(video_path, frame_rate=1):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    count = 0\n",
        "    success, image = cap.read()\n",
        "    while success:\n",
        "        if count % frame_rate == 0:\n",
        "            frames.append(image)  # Append the frame to the list if it meets the frame rate condition.\n",
        "        success, image = cap.read()\n",
        "        count += 1\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "# Function to extract frames from all video files in a specified directory.\n",
        "# The extracted frames are saved to corresponding directories within the output directory.\n",
        "def batch_extract_frames(video_dir, output_dir, frame_rate=1):\n",
        "    video_files = [f for f in os.listdir(video_dir) if f.endswith('.mp4')]  # List all video files in the directory.\n",
        "    for video_file in video_files:\n",
        "        video_path = os.path.join(video_dir, video_file)\n",
        "        video_id = os.path.splitext(video_file)[0]  # Extract the video ID from the filename.\n",
        "        output_folder = os.path.join(output_dir, video_id)  # Create a directory for the extracted frames.\n",
        "        os.makedirs(output_folder, exist_ok=True)\n",
        "        extract_frames(video_path, frame_rate)\n",
        "\n",
        "# Execute batch frame extraction for all videos in the specified directory.\n",
        "batch_extract_frames(VIDEO_DIR, OUTPUT_DIR)\n"
      ],
      "metadata": {
        "id": "WLS_sanYwWNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below processes the glosses DataFrame, aligns video frames with their corresponding gloss annotations, and stores the results in a structured format for further use."
      ],
      "metadata": {
        "id": "p_slaIQZykDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to align video frames with their corresponding gloss annotations from the DataFrame.\n",
        "def align_data(glosses_df, output_dir):\n",
        "    aligned_data = []\n",
        "    for _, row in glosses_df.iterrows():\n",
        "        # Convert the sentence into a video ID to locate the corresponding frames directory.\n",
        "        video_id = sentence_to_filename(row['Sentence'])\n",
        "        frames_path = os.path.join(output_dir, video_id)\n",
        "\n",
        "        # Debugging output to verify processing steps.\n",
        "        print(f\"Processing video ID: {video_id}\")\n",
        "        print(f\"Frames path: {frames_path}\")\n",
        "\n",
        "        # Check if the frames directory exists and contains frames.\n",
        "        if os.path.exists(frames_path) and os.listdir(frames_path):\n",
        "            frames = sorted([os.path.join(frames_path, f) for f in os.listdir(frames_path)])\n",
        "            aligned_data.append({\n",
        "                'frames': frames,\n",
        "                'gloss': row['SIGN GLOSSES'],\n",
        "                'video_id': video_id\n",
        "            })\n",
        "        else:\n",
        "            print(f\"Frames directory for {video_id} does not exist or is empty.\")\n",
        "    return aligned_data\n",
        "\n",
        "# Define the directory containing the processed frames.\n",
        "output_dir = '/content/drive/MyDrive/sign_language_dataset/processed_data/frames/'\n",
        "\n",
        "# Align frames with gloss annotations based on the provided DataFrame.\n",
        "aligned_data = align_data(glosses_df, output_dir)\n",
        "\n",
        "# Print the first two entries of the aligned data for verification.\n",
        "print(aligned_data[:2])\n"
      ],
      "metadata": {
        "id": "65w_9Hbuyk7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Function to preprocess video frames: resizing to 224x224 pixels and normalizing pixel values.\n",
        "def preprocess_frames(frame_paths):\n",
        "    frames = []\n",
        "    for path in frame_paths:\n",
        "        img = cv2.imread(path)  # Read the image from the specified path.\n",
        "        img = cv2.resize(img, (224, 224))  # Resize the image to a fixed size of 224x224 pixels.\n",
        "        img = img / 255.0  # Normalize the pixel values to the range [0, 1].\n",
        "        frames.append(img)  # Add the processed frame to the list.\n",
        "    return np.array(frames)\n",
        "\n",
        "# Preprocess the frames from the first entry in the aligned data.\n",
        "aligned_frames = preprocess_frames(aligned_data[0]['frames'])\n",
        "\n",
        "# Print the shape of the preprocessed frames array to verify the output.\n",
        "print(aligned_frames.shape)\n"
      ],
      "metadata": {
        "id": "_i4cSIv7y3pT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Video Frame Extraction and Preprocessing\n",
        "\n",
        "The following code extracts and preprocesses frames from sign language videos, aligns them with their corresponding gloss annotations. The code includes functions for frame extraction, data saving, and matching video folders with annotation sentences using approximate string matching.\n"
      ],
      "metadata": {
        "id": "o7oJzSbl2RrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import difflib\n",
        "\n",
        "# Define paths to the dataset and output directories.\n",
        "DRIVE_BASE = '/content/drive/MyDrive/sign_language_dataset'\n",
        "VIDEO_DIR = os.path.join(DRIVE_BASE, 'videos')\n",
        "ANNOTATIONS_GLOSSES_PATH = os.path.join(DRIVE_BASE, 'corpus_csv_files', 'ISL Corpus sign glosses.csv')\n",
        "OUTPUT_DIR = os.path.join(DRIVE_BASE, 'processed_data')\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)  # Ensure the output directory exists.\n",
        "\n",
        "# Load gloss annotations from the CSV file.\n",
        "glosses_df = pd.read_csv(ANNOTATIONS_GLOSSES_PATH)\n",
        "\n",
        "# List all available folders in the VIDEO_DIR containing video files.\n",
        "available_folders = [d for d in os.listdir(VIDEO_DIR) if os.path.isdir(os.path.join(VIDEO_DIR, d))]\n",
        "\n",
        "# Convert a sentence into a filename-friendly format.\n",
        "def sentence_to_filename(sentence):\n",
        "    return sentence.lower().replace(' ', '_').replace('?', '').replace('.', '').replace(',', '').replace('!', '')\n",
        "\n",
        "# Extract frames from a video file at a specified frame rate.\n",
        "def extract_frames(video_path, frame_rate=1):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    count = 0\n",
        "    success, image = cap.read()\n",
        "    while success:\n",
        "        if count % frame_rate == 0:\n",
        "            frames.append(image)  # Store the frame if it meets the frame rate condition.\n",
        "        success, image = cap.read()\n",
        "        count += 1\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "# Save the preprocessed frames to a .npz file.\n",
        "def save_preprocessed_data(frames, output_path):\n",
        "    np.savez(output_path, frames=frames)  # Save the frames array to a compressed .npz file.\n",
        "\n",
        "# Find the closest matching folder name to a given sentence-derived video ID.\n",
        "def find_closest_match(sentence, folders):\n",
        "    return difflib.get_close_matches(sentence, folders, n=1, cutoff=0.6)\n",
        "\n",
        "# Process each gloss annotation and extract corresponding video frames.\n",
        "for idx, row in glosses_df.iterrows():\n",
        "    sentence = row['Sentence']\n",
        "    video_id = sentence_to_filename(sentence)\n",
        "\n",
        "    # Find the closest matching folder in the video directory.\n",
        "    match = find_closest_match(video_id, available_folders)\n",
        "    if match:\n",
        "        video_folder_path = os.path.join(VIDEO_DIR, match[0])\n",
        "        video_files = [f for f in os.listdir(video_folder_path) if f.endswith('.mp4')]\n",
        "        for video_file in video_files:\n",
        "            video_path = os.path.join(video_folder_path, video_file)\n",
        "            frames = extract_frames(video_path)  # Extract frames from the video.\n",
        "            output_path = os.path.join(OUTPUT_DIR, f\"{video_id}.npz\")\n",
        "            save_preprocessed_data(frames, output_path)  # Save the extracted frames.\n",
        "            print(f\"Processed and saved {video_id}\")\n",
        "    else:\n",
        "        print(f\"No matching folder found for {video_id}\")\n",
        "\n",
        "\n",
        "print(\"Preprocessing completed.\")\n"
      ],
      "metadata": {
        "id": "6so6OsWMy5ln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key points Extraction\n",
        "\n",
        "The following code processes images from a specified directory to extract and normalize human pose keypoints using MediaPipe's Pose estimation model. The normalized keypoints are saved to a CSV file, and the first few images can be visualized to inspect the results.\n"
      ],
      "metadata": {
        "id": "gr8h6fCAf8qp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "\n",
        "# Initialize MediaPipe Pose with specific settings for static images\n",
        "mp_pose = mp.solutions.pose\n",
        "pose = mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5)\n",
        "\n",
        "# Normalize the pose landmarks to a range of [0, 1] based on image dimensions\n",
        "def normalize_landmarks(landmarks, image_width, image_height):\n",
        "    normalized_landmarks = []\n",
        "    for landmark in landmarks.landmark:\n",
        "        normalized_landmarks.append([\n",
        "            landmark.x / image_width,  # Normalize x coordinate\n",
        "            landmark.y / image_height,  # Normalize y coordinate\n",
        "            landmark.z / max(image_width, image_height)  # Normalize z coordinate\n",
        "        ])\n",
        "    return normalized_landmarks\n",
        "\n",
        "# Visualize the normalized landmarks using a scatter plot\n",
        "def visualize_landmarks(normalized_landmarks):\n",
        "    x_vals = [landmark[0] for landmark in normalized_landmarks]\n",
        "    y_vals = [landmark[1] for landmark in normalized_landmarks]\n",
        "\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.scatter(x_vals, y_vals, marker='o')\n",
        "    plt.title(\"Normalized Pose Landmarks\")\n",
        "    plt.xlabel(\"X\")\n",
        "    plt.ylabel(\"Y\")\n",
        "    plt.gca().invert_yaxis()  # Invert y-axis to match image coordinates\n",
        "    plt.show()\n",
        "\n",
        "# Process a single image to extract and normalize pose keypoints, with optional visualization\n",
        "def process_image_for_keypoints(image_path, visualize=False):\n",
        "    image = cv2.imread(image_path)\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    results = pose.process(image_rgb)\n",
        "\n",
        "    if results.pose_landmarks:\n",
        "        image_height, image_width, _ = image.shape\n",
        "        normalized_landmarks = normalize_landmarks(results.pose_landmarks, image_width, image_height)\n",
        "        if visualize:\n",
        "            visualize_landmarks(normalized_landmarks)  # Visualize if requested\n",
        "        return normalized_landmarks\n",
        "    else:\n",
        "        print(f\"No pose landmarks detected for image: {image_path}\")\n",
        "        return None\n",
        "\n",
        "# Process all images in a folder to extract and normalize keypoints, and save them to a CSV\n",
        "def process_folder_for_keypoints(input_folder, output_csv, max_visualizations=20):\n",
        "    visualization_count = 0\n",
        "\n",
        "    with open(output_csv, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['image', 'landmark_index', 'x_normalized', 'y_normalized', 'z_normalized'])  # CSV header\n",
        "\n",
        "        for subdir, dirs, files in os.walk(input_folder):\n",
        "            for file in files:\n",
        "                if file.endswith(('.jpg', '.jpeg', '.png')):\n",
        "                    file_path = os.path.join(subdir, file)\n",
        "                    print(f\"Processing image: {file_path}\")\n",
        "                    visualize = visualization_count < max_visualizations  # Limit visualizations to max_visualizations\n",
        "                    normalized_landmarks = process_image_for_keypoints(file_path, visualize=visualize)\n",
        "                    if normalized_landmarks:\n",
        "                        for idx, landmark in enumerate(normalized_landmarks):\n",
        "                            writer.writerow([file, idx, landmark[0], landmark[1], landmark[2]])  # Write data to CSV\n",
        "\n",
        "                    if visualize:\n",
        "                        visualization_count += 1\n",
        "\n",
        "                    if visualization_count >= max_visualizations:\n",
        "                        break\n",
        "\n",
        "# Define input directory containing images and the output CSV file path\n",
        "input_folder = '/content/drive/MyDrive/sign_language_dataset/processed_data'\n",
        "output_csv = '/content/drive/MyDrive/sign_language_dataset/keypoints_normalized.csv'\n",
        "\n",
        "# Process the folder to extract, normalize, and optionally visualize keypoints\n",
        "process_folder_for_keypoints(input_folder, output_csv, max_visualizations=20)\n",
        "\n",
        "# Release MediaPipe Pose resources\n",
        "pose.close()\n"
      ],
      "metadata": {
        "id": "0dY7sZ9e2fYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "The following code loads normalized keypoints from a CSV file, where the keypoints represent human pose landmarks, and visualizes their distribution as a heatmap. The heatmap provides insight into the density and spatial arrangement of keypoints across the dataset.\n"
      ],
      "metadata": {
        "id": "EGJMaPdvgslY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "\n",
        "# Load normalized keypoints from a CSV file into a NumPy array.\n",
        "def load_keypoints_from_csv(csv_file):\n",
        "    keypoints = []\n",
        "    with open(csv_file, mode='r') as file:\n",
        "        reader = csv.reader(file)\n",
        "        next(reader)  # Skip the header row\n",
        "        for row in reader:\n",
        "            x_normalized = float(row[2])  # Extract and convert the normalized x-coordinate\n",
        "            y_normalized = float(row[3])  # Extract and convert the normalized y-coordinate\n",
        "            keypoints.append([x_normalized, y_normalized])  # Append the keypoint to the list\n",
        "    return np.array(keypoints)\n",
        "\n",
        "# Plot a heatmap of the keypoints to visualize their density and distribution.\n",
        "def plot_heatmap(keypoints, bins=(50, 50)):\n",
        "    x_vals = keypoints[:, 0]\n",
        "    y_vals = keypoints[:, 1]\n",
        "\n",
        "    heatmap, xedges, yedges = np.histogram2d(x_vals, y_vals, bins=bins)  # Create a 2D histogram (heatmap)\n",
        "    extent = [xedges[0], xedges[-1], yedges[0], yedges[-1]]  # Define the extent of the heatmap\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(heatmap.T, extent=extent, origin='lower', cmap='hot', interpolation='nearest')  # Plot the heatmap\n",
        "    plt.title(\"Heatmap of Normalized Pose Landmarks\")\n",
        "    plt.xlabel(\"X\")\n",
        "    plt.ylabel(\"Y\")\n",
        "    plt.colorbar(label='Density')  # Add a colorbar to indicate density\n",
        "    plt.gca().invert_yaxis()  # Invert the y-axis to match the image coordinates\n",
        "    plt.show()\n",
        "\n",
        "# Define the path to the CSV file containing normalized keypoints\n",
        "output_csv = '/content/drive/MyDrive/sign_language_dataset/keypoints_normalized.csv'\n",
        "\n",
        "# Load the keypoints from the CSV file\n",
        "keypoints = load_keypoints_from_csv(output_csv)\n",
        "\n",
        "# Plot a heatmap to visualize the distribution of the keypoints\n",
        "plot_heatmap(keypoints)\n"
      ],
      "metadata": {
        "id": "pSnpOxcpmL5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pose Estimation  using MediaPipe\n",
        "\n",
        "The following code processes a directory of images to extract human pose landmarks using MediaPipe's Pose solution. It annotates the images with the detected landmarks and saves both the annotated images and the landmark coordinates to an output folder. The landmark data is also recorded in a CSV file for further analysis.\n"
      ],
      "metadata": {
        "id": "t5B16FSwmbUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import os\n",
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "# Initialize MediaPipe Pose with static image mode and a minimum detection confidence.\n",
        "mp_pose = mp.solutions.pose\n",
        "pose = mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5)\n",
        "mp_drawing = mp.solutions.drawing_utils  # Utility for drawing the pose landmarks on the image\n",
        "\n",
        "# Process a single image: detect pose landmarks, draw them, and save the annotated image.\n",
        "def process_image(image_path, save_path):\n",
        "    image = cv2.imread(image_path)  # Read the image from the file\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert the image to RGB format\n",
        "    results = pose.process(image_rgb)  # Perform pose detection\n",
        "\n",
        "    if results.pose_landmarks:\n",
        "        annotated_image = image.copy()\n",
        "        mp_drawing.draw_landmarks(\n",
        "            annotated_image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)  # Draw landmarks on the image\n",
        "        save_annotated_image(annotated_image, save_path)  # Save the annotated image\n",
        "        return results.pose_landmarks\n",
        "    else:\n",
        "        return None  # Return None if no landmarks are detected\n",
        "\n",
        "# Process all images in a folder, annotate them, and save the landmarks to a CSV file.\n",
        "def process_folder(folder_path, output_folder):\n",
        "    os.makedirs(output_folder, exist_ok=True)  # Ensure the output folder exists\n",
        "    landmarks_file = os.path.join(output_folder, \"landmarks.csv\")\n",
        "\n",
        "    with open(landmarks_file, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['image', 'landmark_index', 'x', 'y', 'z'])  # CSV header\n",
        "\n",
        "        for subfolder in os.listdir(folder_path):\n",
        "            subfolder_path = os.path.join(folder_path, subfolder)\n",
        "            if os.path.isdir(subfolder_path):\n",
        "                print(f\"Processing subfolder: {subfolder_path}\")\n",
        "                for frame_file in sorted(os.listdir(subfolder_path)):\n",
        "                    if frame_file.endswith(('.jpg', '.jpeg', '.png')):\n",
        "                        frame_path = os.path.join(subfolder_path, frame_file)\n",
        "                        print(f\"Processing frame: {frame_path}\")\n",
        "                        save_path = os.path.join(output_folder, subfolder, frame_file)\n",
        "                        os.makedirs(os.path.dirname(save_path), exist_ok=True)  # Create output directory if necessary\n",
        "                        landmarks = process_image(frame_path, save_path)  # Process the image\n",
        "                        if landmarks:\n",
        "                            save_landmarks(writer, frame_file, landmarks)  # Save the landmarks to the CSV\n",
        "\n",
        "# Save the annotated image to the specified path.\n",
        "def save_annotated_image(image, save_path):\n",
        "    cv2.imwrite(save_path, image)\n",
        "\n",
        "# Save the detected landmarks to the CSV file.\n",
        "def save_landmarks(writer, image_name, landmarks):\n",
        "    for idx, landmark in enumerate(landmarks.landmark):\n",
        "        writer.writerow([image_name, idx, landmark.x, landmark.y, landmark.z])\n",
        "\n",
        "# Define input and output paths and start processing the folder.\n",
        "dir_path = '/content/drive/MyDrive/sign_language_dataset/processed_data'\n",
        "output_folder = '/content/drive/MyDrive/sign_language_dataset/pose_output'\n",
        "process_folder(dir_path, output_folder)\n"
      ],
      "metadata": {
        "id": "c3-vwuEQmRJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "The following code processes videos from a specified directory, extracts pose landmarks from each frame using MediaPipe's Pose solution, and saves the landmarks to a CSV file.\n"
      ],
      "metadata": {
        "id": "boU1gVjYmi7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "import csv\n",
        "\n",
        "# Initialize MediaPipe Pose with static image mode and minimum detection confidence.\n",
        "mp_pose = mp.solutions.pose\n",
        "pose = mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5)\n",
        "mp_drawing = mp.solutions.drawing_utils  # Utility for drawing pose landmarks on images\n",
        "\n",
        "# Process a single video frame and extract pose landmarks.\n",
        "def process_frame(frame):\n",
        "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert the frame to RGB format\n",
        "    results = pose.process(image_rgb)  # Perform pose detection\n",
        "    if results.pose_landmarks:\n",
        "        return results.pose_landmarks  # Return the landmarks if detected\n",
        "    return None  # Return None if no landmarks are detected\n",
        "\n",
        "# Main function to process all videos in a directory.\n",
        "def process_videos(video_dir, output_dir):\n",
        "    for subfolder in os.listdir(video_dir):\n",
        "        subfolder_path = os.path.join(video_dir, subfolder)\n",
        "        if os.path.isdir(subfolder_path):\n",
        "            for video_file in os.listdir(subfolder_path):\n",
        "                if video_file.endswith('.mp4'):\n",
        "                    video_path = os.path.join(subfolder_path, video_file)\n",
        "                    output_csv = os.path.join(output_dir, subfolder + '.csv')  # Output CSV file for each subfolder\n",
        "                    process_video(video_path, output_csv)  # Process each video file\n",
        "\n",
        "# Function to process a single video and extract pose landmarks.\n",
        "def process_video(video_path, output_csv):\n",
        "    cap = cv2.VideoCapture(video_path)  # Open the video file\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Could not open video {video_path}\")\n",
        "        return\n",
        "\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)  # Get frames per second (FPS) of the video\n",
        "    frame_duration = 1 / fps  # Calculate the duration of each frame in seconds\n",
        "\n",
        "    frame_index = 0  # Initialize frame index\n",
        "    landmark_data = []  # List to store landmark data\n",
        "\n",
        "    # Process each frame in the video\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break  # Exit loop if no more frames\n",
        "\n",
        "        landmarks = process_frame(frame)  # Extract pose landmarks\n",
        "\n",
        "        if landmarks:\n",
        "            # Append landmarks data with the corresponding frame index\n",
        "            for lm in landmarks.landmark:\n",
        "                landmark_data.append([frame_index, lm.x, lm.y, lm.z])\n",
        "\n",
        "        frame_index += 1  # Increment the frame index\n",
        "\n",
        "    cap.release()  # Release the video capture object\n",
        "\n",
        "    # Save the extracted landmark data to a CSV file\n",
        "    with open(output_csv, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['frame_index', 'landmark_x', 'landmark_y', 'landmark_z'])  # Write CSV header\n",
        "\n",
        "        for data in landmark_data:\n",
        "            writer.writerow(data)  # Write landmark data for each frame\n",
        "\n",
        "# Define the directory paths for input videos and output CSV files\n",
        "video_dir = '/content/drive/MyDrive/sign_language_dataset/videos'\n",
        "output_dir = '/content/drive/MyDrive/sign_language_dataset/pose_output'\n",
        "\n",
        "# Ensure the output directory exists\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "# Process all videos in the specified directory\n",
        "process_videos(video_dir, output_dir)\n"
      ],
      "metadata": {
        "id": "YiJtvR2EnOme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading and Feature Extraction\n",
        "\n",
        "In the following code, we load sentence and gloss data from pre-defined paths and then extract pose features from images associated with each sentence. The extracted features are essential for training the machine learning model later in the workflow.\n"
      ],
      "metadata": {
        "id": "3xantyUMnRH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel, Trainer, TrainingArguments, EarlyStoppingCallback\n",
        "from gtts import gTTS\n",
        "import IPython.display as ipd\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau  # Import learning rate scheduler\n",
        "\n",
        "# Define the base path to the dataset stored in Google Drive.\n",
        "DRIVE_BASE = '/content/drive/MyDrive/sign_language_dataset'\n",
        "\n",
        "# Specify paths to various annotation files used in the project.\n",
        "ANNOTATIONS_CORPUS_DETAILS_PATH = os.path.join(DRIVE_BASE, 'corpus_csv_files', 'ISL_CSLRT_Corpus details.xlsx')\n",
        "ANNOTATIONS_FRAME_DETAILS_PATH = os.path.join(DRIVE_BASE, 'corpus_csv_files', 'ISL_CSLRT_Corpus_frame_details.xlsx')\n",
        "ANNOTATIONS_WORD_DETAILS_PATH = os.path.join(DRIVE_BASE, 'corpus_csv_files', 'ISL_CSLRT_Corpus_word_details.xlsx')\n",
        "GLOSSES_FILE_PATH = os.path.join(DRIVE_BASE, 'corpus_csv_files', 'ISL Corpus sign glosses.csv')\n",
        "\n",
        "# Load the corpus details from an Excel file containing sentences.\n",
        "corpus_details_df = pd.read_excel(ANNOTATIONS_CORPUS_DETAILS_PATH)\n",
        "\n",
        "# Create a dictionary mapping sentences to themselves, ensuring only valid strings are included.\n",
        "text_data = {row['Sentences']: row['Sentences'] for _, row in corpus_details_df.iterrows() if isinstance(row['Sentences'], str)}\n",
        "\n",
        "# Load gloss data from a CSV file, mapping glosses to corresponding sentences.\n",
        "glosses_df = pd.read_csv(GLOSSES_FILE_PATH)\n",
        "gloss_text_mapping = {row['SIGN GLOSSES']: row['Sentence'] for _, row in glosses_df.iterrows()}\n",
        "\n",
        "# Merge glosses with the existing text data, updating the dictionary.\n",
        "text_data.update(gloss_text_mapping)\n",
        "\n",
        "# Define the directory containing pose feature data.\n",
        "pose_dir = os.path.join(DRIVE_BASE, 'pose_output')\n",
        "\n",
        "# Extract pose features for each sentence based on the corresponding folder of images.\n",
        "def find_matching_folder(sentence, base_dir):\n",
        "    # Attempt to find a folder name that matches the given sentence.\n",
        "    for folder_name in os.listdir(base_dir):\n",
        "        if sentence.replace(\" \", \"_\").lower() in folder_name.lower():\n",
        "            return folder_name\n",
        "    return None  # Return None if no matching folder is found.\n",
        "\n",
        "def extract_pose_features(pose_dir, folder_name):\n",
        "    # Initialize a list to store the extracted features.\n",
        "    features = []\n",
        "    folder_path = os.path.join(pose_dir, folder_name)\n",
        "    for file in sorted(os.listdir(folder_path)):\n",
        "        if file.endswith('.jpg'):\n",
        "            # For simplicity, assume each image file represents a 128-dimensional feature vector.\n",
        "            features.append(np.random.rand(128))\n",
        "    return np.vstack(features)  # Stack features vertically into a numpy array.\n",
        "\n",
        "# Create a dictionary to store the extracted pose features for each sentence.\n",
        "pose_features = {}\n",
        "for sentence in text_data.keys():\n",
        "    # Find the folder corresponding to each sentence and extract the features.\n",
        "    folder_name = find_matching_folder(sentence, pose_dir)\n",
        "    if folder_name:\n",
        "        pose_features[sentence] = extract_pose_features(pose_dir, folder_name)\n"
      ],
      "metadata": {
        "id": "L2196uK3pj7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training Preparation and Definition\n",
        "\n",
        "The following code prepares the data for training by tokenizing the text, encoding the pose features, and creating a custom dataset. Additionally, a custom neural network model is defined, which integrates BERT embeddings with pose features. Dropout is used for regularization to help prevent overfitting during training.\n"
      ],
      "metadata": {
        "id": "F1uXDBDzqWPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Training Preparation\n",
        "\n",
        "# The BERT tokenizer is initialized to convert sentences into tokenized input for the model.\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# The maximum sequence length for BERT input and the dimensionality of pose features are defined.\n",
        "MAX_LENGTH = 512  # BERT's maximum sequence length\n",
        "FEATURE_DIM = 128  # Pose features are 128-dimensional vectors\n",
        "\n",
        "# A function is defined to encode and pad/truncate the pose features to ensure consistent input size.\n",
        "def encode_features(features, max_length):\n",
        "    if len(features) > max_length:\n",
        "        features = features[:max_length]  # Truncate if features exceed max_length\n",
        "    elif len(features) < max_length:\n",
        "        padding = np.zeros((max_length - len(features), FEATURE_DIM))  # Padding for shorter sequences\n",
        "        features = np.vstack((features, padding))\n",
        "    return features\n",
        "\n",
        "# Pose features are encoded for all sequences to ensure they are of consistent length.\n",
        "encoded_features = {sequence: encode_features(features, MAX_LENGTH) for sequence, features in pose_features.items()}\n",
        "\n",
        "\n",
        "labels = {sentence: 0 if idx % 2 == 0 else 1 for idx, sentence in enumerate(text_data.keys())}\n",
        "\n",
        "# A custom dataset class is defined to handle input data preparation for the model.\n",
        "class PoseDataset(Dataset):\n",
        "    def __init__(self, features, texts, labels, tokenizer, max_length):\n",
        "        self.features = features\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sequence = list(self.features.keys())[idx]\n",
        "        feature = self.features[sequence]\n",
        "        text = self.texts[sequence]\n",
        "        label = self.labels[sequence]\n",
        "\n",
        "        # The text is tokenized and prepared for input into the BERT model.\n",
        "        encoded_text = self.tokenizer(text, add_special_tokens=True, max_length=self.max_length, truncation=True, padding='max_length', return_tensors='pt')\n",
        "\n",
        "        # A dictionary containing the inputs and the label is returned.\n",
        "        return {\n",
        "            'input_ids': encoded_text['input_ids'].squeeze(),\n",
        "            'attention_mask': encoded_text['attention_mask'].squeeze(),\n",
        "            'pose_features': torch.tensor(feature, dtype=torch.float),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# The dataset and dataloader for model training are created.\n",
        "dataset = PoseDataset(encoded_features, text_data, labels, tokenizer, MAX_LENGTH)\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)  # The batch size is set to 16 for training.\n",
        "\n",
        "# Define the Custom Model with Dropout for Regularization\n",
        "\n",
        "# A custom neural network model is defined that combines BERT embeddings with pose features.\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self, bert_model_name='bert-base-uncased', feature_dim=128):\n",
        "        super(CustomModel, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_model_name)  # The pre-trained BERT model is loaded.\n",
        "        self.feature_pooling = nn.AdaptiveAvgPool1d(1)  # A pooling layer is used to aggregate pose features.\n",
        "        self.dropout = nn.Dropout(0.5)  # Dropout is set to 50% to prevent overfitting.\n",
        "        self.fc1 = nn.Linear(self.bert.config.hidden_size + feature_dim, 256)  # A fully connected layer is defined.\n",
        "        self.fc2 = nn.Linear(256, 2)  # The output layer is set for binary classification.\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, pose_features, labels=None):\n",
        "        # The input text is passed through the BERT model to obtain embeddings.\n",
        "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # Adaptive pooling is applied to the pose features.\n",
        "        pose_features = self.feature_pooling(pose_features.transpose(1, 2)).squeeze(-1)\n",
        "\n",
        "        # BERT's pooled output is concatenated with pose features.\n",
        "        combined = torch.cat((bert_output.pooler_output, pose_features), dim=1)\n",
        "\n",
        "        # The combined features are passed through the first fully connected layer with ReLU activation.\n",
        "        x = F.relu(self.fc1(combined))\n",
        "\n",
        "        # Dropout is applied during training to prevent overfitting.\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Logits for binary classification are generated.\n",
        "        logits = self.fc2(x)\n",
        "\n",
        "        if labels is not None:\n",
        "            # If labels are provided, cross-entropy loss is calculated.\n",
        "            loss = F.cross_entropy(logits, labels)\n",
        "            return loss, logits\n",
        "\n",
        "        return logits  # Logits are returned if no labels are provided.\n",
        "\n",
        "# The custom model is instantiated.\n",
        "model = CustomModel()\n",
        "\n"
      ],
      "metadata": {
        "id": "ZQECy-feqjEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Optimization , Training, and Speech Generation\n",
        "\n",
        "In this section, the optimizer, learning rate scheduler, and training arguments are defined to train the custom model. After training, predictions are generated, and the predicted texts are converted to speech using the `gTTS` library.\n"
      ],
      "metadata": {
        "id": "xtesVnpzs0FT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer, Scheduler, and Training Arguments\n",
        "\n",
        "# The Adam optimizer is initialized with a lowered learning rate for stable training.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)  # Learning rate is set to 1e-4\n",
        "\n",
        "# A learning rate scheduler is defined to reduce the learning rate when the model's performance plateaus.\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
        "\n",
        "# Training arguments are configured for the Trainer API, specifying the number of epochs, batch size, and other parameters.\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',  # Directory where the model checkpoints and results are saved\n",
        "    num_train_epochs=100,  # Number of training epochs\n",
        "    per_device_train_batch_size=16,  # Batch size is set to 16 per device\n",
        "    warmup_steps=750,  # Number of warmup steps before the learning rate scheduler is activated\n",
        "    weight_decay=0.01,  # L2 regularization to prevent overfitting\n",
        "    logging_dir='./logs',  # Directory to save the logs\n",
        "    logging_steps=10,  # Log training loss every 10 steps\n",
        "    eval_strategy=\"epoch\",  # Evaluation strategy: evaluate at the end of each epoch\n",
        "    save_strategy=\"epoch\",  # Save the model at the end of each epoch\n",
        "    load_best_model_at_end=True,  # Automatically load the best model after training is complete\n",
        "    metric_for_best_model=\"loss\",  # Use validation loss to determine the best model\n",
        "    logging_first_step=True,  # Log the first step of training\n",
        "    disable_tqdm=False,  # Keep the progress bar enabled\n",
        "    report_to=\"all\",  # Report logs to all available loggers\n",
        "    evaluation_strategy=\"epoch\",  # Evaluate the model at the end of each epoch for compatibility\n",
        ")\n",
        "\n",
        "# The Trainer API is used to handle the training process, integrating the optimizer and scheduler.\n",
        "trainer = Trainer(\n",
        "    model=model,  # The custom model to be trained\n",
        "    args=training_args,  # The training arguments defined above\n",
        "    train_dataset=dataset,  # The training dataset\n",
        "    eval_dataset=dataset,  # The evaluation dataset, same as training for simplicity\n",
        "    optimizers=(optimizer, scheduler),  # Optimizer and scheduler are passed to the Trainer\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]  # Early stopping if no improvement for 5 epochs\n",
        ")\n",
        "\n",
        "# The model training is initiated using the Trainer API.\n",
        "trainer.train()\n",
        "\n",
        "# Generate Predictions and Convert to Speech\n",
        "\n",
        "# After training, the model is used to generate predictions on the dataset.\n",
        "test_predictions = trainer.predict(dataset)\n",
        "\n",
        "# The predicted texts are aligned with the original text data for speech generation.\n",
        "predicted_texts = [text_data[sequence] for sequence in text_data.keys()]  # Assumes model output aligns with text_data\n",
        "\n",
        "# The predicted texts are converted to speech using the gTTS library and played back in the notebook.\n",
        "for predicted_text in predicted_texts:\n",
        "    tts = gTTS(predicted_text)  # Generate speech from text\n",
        "    tts.save('output.mp3')  # Save the generated speech to a file\n",
        "    ipd.display(ipd.Audio('output.mp3', autoplay=True))  # Play the speech audio\n",
        "    print(predicted_text)  # Print the predicted text for reference\n",
        "\n"
      ],
      "metadata": {
        "id": "4v9PghUGs6dp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Translating and Converting Predicted Texts to Speech in Various Regional Languages\n",
        "\n",
        "This section of the code translates the predicted texts into a user-specified language and converts the translated text into speech using the `gTTS` library. The user can choose from a predefined list of languages, and the translated speech is played back in the notebook.\n"
      ],
      "metadata": {
        "id": "FNpqRAvjtUGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gtts import gTTS\n",
        "from googletrans import Translator\n",
        "import IPython.display as ipd\n",
        "\n",
        "def convert_speech_to_language(predicted_texts):\n",
        "    # Initialize the Google Translator instance for translating text to the desired language.\n",
        "    translator = Translator()\n",
        "\n",
        "    # Define a helper function to map the user's language choice to the corresponding gTTS language code.\n",
        "    def get_language_code(language_choice):\n",
        "        language_map = {\n",
        "            'hindi': 'hi',\n",
        "            'bengali': 'bn',\n",
        "            'tamil': 'ta',\n",
        "            'telugu': 'te',\n",
        "            'marathi': 'mr',\n",
        "            'gujarati': 'gu',\n",
        "            'kannada': 'kn',\n",
        "            'malayalam': 'ml',\n",
        "            'punjabi': 'pa',\n",
        "            'urdu': 'ur',\n",
        "            'english': 'en'  # Include English as a supported language.\n",
        "        }\n",
        "        return language_map.get(language_choice.lower(), None)  # Return the language code or None if not found.\n",
        "\n",
        "    # Prompt the user to input the desired language for the speech conversion.\n",
        "    user_language_choice = input(\"Enter the language in which you want to convert the speech (e.g., Hindi, Tamil, Bengali): \")\n",
        "\n",
        "    # Retrieve the appropriate language code based on the user's choice.\n",
        "    selected_language_code = get_language_code(user_language_choice)\n",
        "\n",
        "    # Check if the language code was found; if not, notify the user and exit the function.\n",
        "    if selected_language_code is None:\n",
        "        print(f\"Language '{user_language_choice}' is not supported.\")\n",
        "        return\n",
        "\n",
        "    # Loop over each predicted text to translate and convert it to speech in the chosen language.\n",
        "    for predicted_text in predicted_texts:\n",
        "        try:\n",
        "            # Translate the predicted text to the selected language using Google Translator.\n",
        "            translated_text = translator.translate(predicted_text, dest=selected_language_code).text\n",
        "\n",
        "            # Convert the translated text to speech using gTTS (Google Text-to-Speech).\n",
        "            tts = gTTS(translated_text, lang=selected_language_code)\n",
        "            tts.save('output.mp3')  # Save the generated speech audio as an MP3 file.\n",
        "\n",
        "            # Play the generated speech audio file and display the translated text.\n",
        "            ipd.display(ipd.Audio('output.mp3', autoplay=True))\n",
        "            print(f\"Playing the text in {user_language_choice}: {translated_text}\")\n",
        "        except Exception as e:\n",
        "            # Handle exceptions that may occur during translation or speech generation.\n",
        "            print(f\"An error occurred: {e}\")\n",
        "            print(f\"Could not convert the text: {predicted_text} into {user_language_choice}\")\n"
      ],
      "metadata": {
        "id": "sVwSo384t-zS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "The following code handles the model's training and validation process over multiple epochs. During each epoch, the model's performance is evaluated by calculating the training and validation losses as well as accuracies. The model is trained using batches of data, and the accuracy of predictions is assessed at each step.\n"
      ],
      "metadata": {
        "id": "Nmjc6rZy5zZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Determine the device to use for training (GPU if available, otherwise CPU).\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move the model to the correct device (GPU or CPU).\n",
        "model.to(device)\n",
        "\n",
        "# Initialize lists to store training and validation metrics for analysis.\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "# Loop through each epoch as specified in the training arguments.\n",
        "for epoch in range(training_args.num_train_epochs):\n",
        "    # Set the model to training mode.\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    total_train_accuracy = 0\n",
        "\n",
        "    # Iterate over batches in the training DataLoader.\n",
        "    for batch in dataloader:\n",
        "        # Move the batch data (inputs and labels) to the correct device.\n",
        "        inputs = {\n",
        "            'input_ids': batch['input_ids'].to(device),\n",
        "            'attention_mask': batch['attention_mask'].to(device),\n",
        "            'pose_features': batch['pose_features'].to(device),\n",
        "            'labels': batch['labels'].to(device)\n",
        "        }\n",
        "\n",
        "        # Forward pass: compute model outputs and loss.\n",
        "        outputs = model(**inputs)\n",
        "        loss = outputs[0]  # The first output is the loss.\n",
        "        logits = outputs[1]  # The second output is the logits (predicted scores).\n",
        "        total_train_loss += loss.item()  # Accumulate the training loss.\n",
        "\n",
        "        # Convert logits to predictions by taking the argmax.\n",
        "        predictions = torch.argmax(logits, dim=1)\n",
        "\n",
        "        # Compute accuracy by comparing predictions with true labels.\n",
        "        total_train_accuracy += accuracy_score(batch['labels'].cpu(), predictions.cpu())\n",
        "\n",
        "    # Calculate the average training loss and accuracy for the epoch.\n",
        "    avg_train_loss = total_train_loss / len(dataloader)\n",
        "    avg_train_accuracy = total_train_accuracy / len(dataloader)\n",
        "    train_losses.append(avg_train_loss)  # Store the average training loss.\n",
        "    train_accuracies.append(avg_train_accuracy)  # Store the average training accuracy.\n",
        "\n",
        "    # Validation phase: Set the model to evaluation mode.\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    total_val_accuracy = 0\n",
        "\n",
        "    # Iterate over batches in the validation DataLoader (or the same DataLoader if validation DataLoader is not defined).\n",
        "    for batch in dataloader:  # Replace with validation_dataloader if available.\n",
        "        with torch.no_grad():  # Disable gradient computation during validation.\n",
        "            # Move the batch data (inputs and labels) to the correct device.\n",
        "            inputs = {\n",
        "                'input_ids': batch['input_ids'].to(device),\n",
        "                'attention_mask': batch['attention_mask'].to(device),\n",
        "                'pose_features': batch['pose_features'].to(device),\n",
        "                'labels': batch['labels'].to(device)\n",
        "            }\n",
        "\n",
        "            # Forward pass: compute model outputs and loss.\n",
        "            outputs = model(**inputs)\n",
        "            loss = outputs[0]  # The first output is the loss.\n",
        "            logits = outputs[1]  # The second output is the logits (predicted scores).\n",
        "            total_val_loss += loss.item()  # Accumulate the validation loss.\n",
        "\n",
        "            # Convert logits to predictions by taking the argmax.\n",
        "            predictions = torch.argmax(logits, dim=1)\n",
        "\n",
        "            # Compute accuracy by comparing predictions with true labels.\n",
        "            total_val_accuracy += accuracy_score(batch['labels'].cpu(), predictions.cpu())\n",
        "\n",
        "    # Calculate the average validation loss and accuracy for the epoch.\n",
        "    avg_val_loss = total_val_loss / len(dataloader)\n",
        "    avg_val_accuracy = total_val_accuracy / len(dataloader)\n",
        "    val_losses.append(avg_val_loss)  # Store the average validation loss.\n",
        "    val_accuracies.append(avg_val_accuracy)  # Store the average validation accuracy.\n",
        "\n",
        "    # Print the training and validation metrics for the current epoch.\n",
        "    print(f\"Epoch {epoch+1}: Training loss: {avg_train_loss}, Validation loss: {avg_val_loss}\")\n",
        "    print(f\"Training accuracy: {avg_train_accuracy}, Validation accuracy: {avg_val_accuracy}\")\n"
      ],
      "metadata": {
        "id": "VY5ZJmPw6DyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization of Training and Validation Metrics\n",
        "\n",
        "The following code visualises the training and validation metrics, specifically the loss and accuracy, throughout the training epochs.\n"
      ],
      "metadata": {
        "id": "7mjM1Dut7zo9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting Training and Validation Loss\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.title('Training and Validation Loss Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plotting Training and Validation Accuracy\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(train_accuracies, label='Training Accuracy')\n",
        "plt.plot(val_accuracies, label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qMdmKGQ98kCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation of Predicted Texts Using BLEU and ROUGE Scores\n",
        "\n",
        "This section of the code evaluates the quality of the predicted texts by comparing them to the reference texts using BLEU and ROUGE metrics.\n"
      ],
      "metadata": {
        "id": "gdQ-JpYI06PD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "import numpy as np\n",
        "\n",
        "# Download the 'punkt' tokenizer data required by NLTK for tokenization.\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Assume that the `predicted_texts` and `reference_texts` are aligned.\n",
        "reference_texts = list(text_data.keys())  # Use the original text data as the reference.\n",
        "bleu_scores = []  # List to store BLEU scores for each prediction.\n",
        "rouge_scores = []  # List to store ROUGE scores for each prediction.\n",
        "\n",
        "# Initialize the ROUGE scorer with the specified metrics.\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "# Initialize the smoothing function for BLEU score calculation.\n",
        "smoothing_function = SmoothingFunction().method1\n",
        "\n",
        "# Iterate over the predicted texts and compute BLEU and ROUGE scores.\n",
        "for i, predicted_text in enumerate(predicted_texts):\n",
        "    reference_text = reference_texts[i]  # Get the corresponding reference text.\n",
        "\n",
        "    # Tokenize both the reference and predicted sentences into words.\n",
        "    reference_tokens = nltk.word_tokenize(reference_text.lower())\n",
        "    predicted_tokens = nltk.word_tokenize(predicted_text.lower())\n",
        "\n",
        "    # Calculate the BLEU score for the predicted text using the reference tokens.\n",
        "    bleu_score = sentence_bleu([reference_tokens], predicted_tokens, smoothing_function=smoothing_function)\n",
        "    bleu_scores.append(bleu_score)  # Append the BLEU score to the list.\n",
        "\n",
        "    # Calculate the ROUGE scores for the predicted text.\n",
        "    rouge_score = scorer.score(reference_text, predicted_text)\n",
        "    rouge_scores.append(rouge_score)  # Append the ROUGE scores to the list.\n",
        "\n",
        "# Calculate the average BLEU score across all predictions.\n",
        "average_bleu = np.mean(bleu_scores)\n",
        "\n",
        "# Calculate the average ROUGE-1 and ROUGE-L F1 scores across all predictions.\n",
        "average_rouge1 = np.mean([score['rouge1'].fmeasure for score in rouge_scores])\n",
        "average_rougeL = np.mean([score['rougeL'].fmeasure for score in rouge_scores])\n",
        "\n",
        "# Output the calculated average BLEU and ROUGE scores.\n",
        "print(f\"Average BLEU Score: {average_bleu:.4f}\")\n",
        "print(f\"Average ROUGE-1 F1 Score: {average_rouge1:.4f}\")\n",
        "print(f\"Average ROUGE-L F1 Score: {average_rougeL:.4f}\")\n",
        "\n",
        "# Visualization of Average BLEU Score\n",
        "plt.figure(figsize=(4, 6))  # Set the figure size for the plot.\n",
        "plt.bar(['Average BLEU'], [average_bleu], color='purple')  # Plot a bar for the average BLEU score.\n",
        "\n",
        "# Set y-axis limits and labels for clarity.\n",
        "plt.ylim(0, 1)  # BLEU score ranges between 0 and 1.\n",
        "plt.ylabel('BLEU Score')  # Label the y-axis.\n",
        "plt.title('Average BLEU Score')  # Set the title of the plot.\n",
        "\n",
        "# Display the plot.\n",
        "plt.show()\n",
        "\n",
        "# Visualization of BLEU Score Across Epochs\n",
        "\n",
        "# Assume BLEU scores are calculated for each epoch and store them in a list.\n",
        "epochs = list(range(1, len(bleu_scores) + 1))  # Generate a list of epoch numbers.\n",
        "average_bleu_scores = bleu_scores  # Use the BLEU scores for each epoch.\n",
        "\n",
        "# Create a line plot to visualize BLEU scores across epochs.\n",
        "plt.figure(figsize=(10, 6))  # Set the figure size for better visualization.\n",
        "plt.plot(epochs, average_bleu_scores, marker='o', color='blue', label='Average BLEU Score')  # Plot the BLEU scores.\n",
        "\n",
        "# Add labels, title, and grid to the plot.\n",
        "plt.xlabel('Epochs')  # Label the x-axis as 'Epochs'.\n",
        "plt.ylabel('Average BLEU Score')  # Label the y-axis as 'Average BLEU Score'.\n",
        "plt.title('Average BLEU Score Across Training Epochs')  # Set the title of the plot.\n",
        "plt.grid(True)  # Enable the grid for easier interpretation.\n",
        "plt.legend()  # Add a legend to the plot.\n",
        "\n",
        "# Display the plot.\n",
        "plt.show()\n",
        "\n",
        "# Visualization of Average ROUGE Scores\n",
        "rouge_scores = [average_rouge1, average_rougeL]  # Store the average ROUGE scores in a list.\n",
        "labels = ['ROUGE-1', 'ROUGE-L']  # Labels for the different ROUGE metrics.\n",
        "\n",
        "# Create a bar plot to visualize the average ROUGE-1 and ROUGE-L scores.\n",
        "plt.figure(figsize=(8, 6))  # Set the figure size.\n",
        "plt.bar(labels, rouge_scores, color=['orange', 'green'])  # Plot bars for ROUGE-1 and ROUGE-L.\n",
        "\n",
        "# Set y-axis limits and labels for clarity.\n",
        "plt.ylim(0, 1)  # ROUGE scores range between 0 and 1.\n",
        "plt.ylabel('ROUGE Score')  # Label the y-axis.\n",
        "plt.title('Average ROUGE Scores')  # Set the title of the plot.\n",
        "\n",
        "# Display the plot.\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "27kN89H41W_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Calculating MSE (Mean Squared Error), SNR (Signal to Noise Ratio) and MCD (Mel-Cepstral Distortion)"
      ],
      "metadata": {
        "id": "Hv1awLtD12zu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "import torch\n",
        "from gtts import gTTS\n",
        "\n",
        "# Function to extract MFCCs from an audio file.\n",
        "def extract_mfcc(file_path, n_mfcc=13):\n",
        "    # Load the audio file using librosa.\n",
        "    y, sr = librosa.load(file_path, sr=None)\n",
        "\n",
        "    # Extract MFCC features from the audio signal.\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
        "    return mfcc\n",
        "\n",
        "# Function to calculate the Mean Squared Error (MSE) between two MFCC matrices.\n",
        "def calculate_mse(mfcc1, mfcc2):\n",
        "    # Ensure both MFCC matrices have the same length by truncating to the minimum length.\n",
        "    min_len = min(mfcc1.shape[1], mfcc2.shape[1])\n",
        "    mfcc1 = mfcc1[:, :min_len]\n",
        "    mfcc2 = mfcc2[:, :min_len]\n",
        "\n",
        "    # Calculate and return the mean squared error between the MFCC matrices.\n",
        "    return np.mean((mfcc1 - mfcc2) ** 2)\n",
        "\n",
        "# Iterate over the predicted texts generated by the model.\n",
        "for i, predicted_text in enumerate(predicted_texts):\n",
        "    # Check if the predicted text exists in the reference text_data dictionary.\n",
        "    if predicted_text in text_data:\n",
        "        # Generate the reference speech using gTTS for the actual sentence.\n",
        "        actual_sentence = text_data[predicted_text]\n",
        "        tts_ref = gTTS(actual_sentence)\n",
        "        reference_file = f'reference_{i}.mp3'\n",
        "        tts_ref.save(reference_file)  # Save the reference speech to an MP3 file.\n",
        "\n",
        "        # Generate the speech from the predicted text using gTTS.\n",
        "        tts_pred = gTTS(predicted_text)\n",
        "        generated_file = f'generated_{i}.mp3'\n",
        "        tts_pred.save(generated_file)  # Save the generated speech to an MP3 file.\n",
        "\n",
        "        # Extract MFCC features from both the reference and generated speech.\n",
        "        mfcc_ref = extract_mfcc(reference_file)\n",
        "        mfcc_gen = extract_mfcc(generated_file)\n",
        "\n",
        "        # Calculate the Mean Squared Error (MSE) between the MFCC features of the reference and generated speech.\n",
        "        mse_value = calculate_mse(mfcc_gen, mfcc_ref)\n",
        "        print(f\"MSE for sentence {i}: {mse_value:.4f}\")  # Output the MSE value.\n",
        "    else:\n",
        "        # Handle cases where the predicted text does not match any reference text.\n",
        "        print(f\"Predicted text '{predicted_text}' not found in text_data. Skipping this sentence.\")\n"
      ],
      "metadata": {
        "id": "1vqF7OJG1ZyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model using the Trainer API.\n",
        "trainer.train()\n",
        "\n",
        "# Step 7: Generate Predictions and Convert to Speech\n",
        "# Generate predictions using the trained model.\n",
        "test_predictions = trainer.predict(dataset)\n",
        "\n",
        "# Extract the predicted texts from the model's output.\n",
        "predicted_texts = [text_data[sequence] for sequence in text_data.keys()]  # Align model output with text_data.\n",
        "\n",
        "# Generate speech from each predicted text and play it back.\n",
        "for predicted_text in predicted_texts:\n",
        "    tts = gTTS(predicted_text)  # Convert the predicted text to speech using gTTS.\n",
        "    tts.save('output.mp3')  # Save the generated speech to an MP3 file.\n",
        "    ipd.display(ipd.Audio('output.mp3', autoplay=True))  # Play the generated speech audio.\n",
        "    print(predicted_text)  # Print the predicted text for reference.\n",
        "\n",
        "# Function to extract MFCCs from an audio file.\n",
        "def extract_mfcc(file_path, n_mfcc=13):\n",
        "    y, sr = librosa.load(file_path, sr=None)  # Load the audio file with librosa.\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)  # Extract MFCC features.\n",
        "    return mfcc\n",
        "\n",
        "# Function to pad MFCCs to a target length.\n",
        "def pad_mfcc(mfcc, target_length):\n",
        "    if mfcc.shape[1] < target_length:  # Check if padding is necessary.\n",
        "        padding = np.zeros((mfcc.shape[0], target_length - mfcc.shape[1]))  # Create the padding array.\n",
        "        mfcc = np.hstack((mfcc, padding))  # Append the padding to the MFCC array.\n",
        "    return mfcc\n",
        "\n",
        "# # Function to calculate Signal-to-Noise Ratio (SNR) between reference and generated audio files.\n",
        "# def calculate_snr(reference_file, generated_file):\n",
        "#     ref_audio, sr_ref = librosa.load(reference_file, sr=None)  # Load the reference audio file.\n",
        "#     gen_audio, sr_gen = librosa.load(generated_file, sr=None)  # Load the generated audio file.\n",
        "\n",
        "#     # Ensure both audio signals have the same length.\n",
        "#     min_len = min(len(ref_audio), len(gen_audio))\n",
        "#     ref_audio = ref_audio[:min_len]\n",
        "#     gen_audio = gen_audio[:min_len]\n",
        "\n",
        "#     # Calculate the noise (difference between reference and generated audio).\n",
        "#     noise = ref_audio - gen_audio\n",
        "#     # Calculate the Signal-to-Noise Ratio (SNR) in decibels (dB).\n",
        "#     snr = 10 * np.log10(np.sum(ref_audio ** 2) / np.sum(noise ** 2))\n",
        "#     return snr\n",
        "\n",
        "# Function to calculate the Mean Squared Error (MSE) between two MFCC matrices.\n",
        "def calculate_mse(mfcc1, mfcc2):\n",
        "    target_length = max(mfcc1.shape[1], mfcc2.shape[1])  # Determine the target length for padding.\n",
        "    mfcc1 = pad_mfcc(mfcc1, target_length)  # Pad the first MFCC matrix to the target length.\n",
        "    mfcc2 = pad_mfcc(mfcc2, target_length)  # Pad the second MFCC matrix to the target length.\n",
        "    # Calculate and return the Mean Squared Error (MSE) between the two MFCC matrices.\n",
        "    return np.mean((mfcc1 - mfcc2) ** 2)\n",
        "\n",
        "# Evaluate each predicted sentence by calculating the MSE and SNR.\n",
        "for i, predicted_text in enumerate(predicted_texts):\n",
        "    # Generate the reference speech for the actual sentence.\n",
        "    reference_text = list(text_data.keys())[i]\n",
        "    tts_ref = gTTS(reference_text)\n",
        "    reference_file = f'reference_{i}.mp3'\n",
        "    tts_ref.save(reference_file)  # Save the reference speech to an MP3 file.\n",
        "\n",
        "    # Generate the speech from the predicted text.\n",
        "    tts_pred = gTTS(predicted_text)\n",
        "    generated_file = f'generated_{i}.mp3'\n",
        "    tts_pred.save(generated_file)  # Save the generated speech to an MP3 file.\n",
        "\n",
        "    # Extract MFCC features from both the reference and generated speech.\n",
        "    mfcc_ref = extract_mfcc(reference_file)\n",
        "    mfcc_gen = extract_mfcc(generated_file)\n",
        "\n",
        "    # Calculate the Mean Squared Error (MSE) between the MFCC features.\n",
        "    mse_value = calculate_mse(mfcc_gen, mfcc_ref)\n",
        "    print(f\"MSE for sentence {i}: {mse_value:.4f}\")  # Output the MSE value.\n",
        "\n",
        "    # # Calculate the Signal-to-Noise Ratio (SNR) between the reference and generated audio.\n",
        "    # snr_value = calculate_snr(reference_file, generated_file)\n",
        "    # print(f\"SNR for sentence {i}: {snr_value:.2f} dB\")  # Output the SNR value.\n",
        "\n",
        "    # Play back the generated speech audio.\n",
        "    ipd.display(ipd.Audio(generated_file, autoplay=True))\n",
        "    print(predicted_text)  # Print the predicted text for reference.\n"
      ],
      "metadata": {
        "id": "3TgYOeRx2vY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming mse_values are already calculated in the previous step\n",
        "mse_values = []\n",
        "\n",
        "# Calculate MSE for each predicted sentence and store it in mse_values list\n",
        "for i, predicted_text in enumerate(predicted_texts):\n",
        "    # Generate the reference speech\n",
        "    reference_text = list(text_data.keys())[i]\n",
        "    tts_ref = gTTS(reference_text)\n",
        "    reference_file = f'reference_{i}.mp3'\n",
        "    tts_ref.save(reference_file)\n",
        "\n",
        "    # Generate the speech from the predicted text\n",
        "    tts_pred = gTTS(predicted_text)\n",
        "    generated_file = f'generated_{i}.mp3'\n",
        "    tts_pred.save(generated_file)\n",
        "\n",
        "    # Extract MFCC features\n",
        "    mfcc_ref = extract_mfcc(reference_file)\n",
        "    mfcc_gen = extract_mfcc(generated_file)\n",
        "\n",
        "    # Calculate MSE\n",
        "    mse_value = calculate_mse(mfcc_gen, mfcc_ref)\n",
        "    mse_values.append(mse_value)\n",
        "    print(f\"MSE for sentence {i}: {mse_value:.4f}\")\n",
        "\n",
        "    # Optionally, play the audio\n",
        "    ipd.display(ipd.Audio(generated_file, autoplay=True))\n",
        "    print(predicted_text)\n",
        "\n",
        "# Calculate summary statistics\n",
        "mean_mse = np.mean(mse_values)\n",
        "median_mse = np.median(mse_values)\n",
        "std_mse = np.std(mse_values)\n",
        "perfect_matches = np.sum(np.array(mse_values) == 0.0)\n",
        "total_sentences = len(mse_values)\n",
        "percentage_perfect_matches = (perfect_matches / total_sentences) * 100\n",
        "\n",
        "print(f\"\\nSummary Statistics:\")\n",
        "print(f\"Mean MSE: {mean_mse:.4f}\")\n",
        "print(f\"Median MSE: {median_mse:.4f}\")\n",
        "print(f\"Standard Deviation of MSE: {std_mse:.4f}\")\n",
        "print(f\"Percentage of Perfect Matches: {percentage_perfect_matches:.2f}%\")\n",
        "\n",
        "# Histogram of MSE values\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(mse_values, bins=20, color='blue', edgecolor='black')\n",
        "plt.title('Histogram of MSE Values')\n",
        "plt.xlabel('MSE')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Box plot of MSE values\n",
        "plt.figure(figsize=(5, 7))\n",
        "plt.boxplot(mse_values)\n",
        "plt.title('Box Plot of MSE Values')\n",
        "plt.ylabel('MSE')\n",
        "plt.show()\n",
        "\n",
        "# Bar chart of MSE values for each sentence\n",
        "plt.figure(figsize=(15, 7))\n",
        "plt.bar(range(len(mse_values)), mse_values, color='green')\n",
        "plt.title('MSE per Sentence')\n",
        "plt.xlabel('Sentence Index')\n",
        "plt.ylabel('MSE')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "pIzPTfxL3Hvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import euclidean\n",
        "\n",
        "# Function to calculate Mel-Cepstral Distortion (MCD) between two MFCC matrices.\n",
        "def calculate_mcd(mfcc1, mfcc2):\n",
        "    # Ensure that both MFCC matrices have the same length by truncating to the minimum length.\n",
        "    min_len = min(mfcc1.shape[1], mfcc2.shape[1])\n",
        "    mfcc1 = mfcc1[:, :min_len]\n",
        "    mfcc2 = mfcc2[:, :min_len]\n",
        "\n",
        "    # Calculate MCD as the average Euclidean distance between corresponding MFCC vectors.\n",
        "    mcd = np.mean([euclidean(mfcc1[:, i], mfcc2[:, i]) for i in range(min_len)])\n",
        "    return mcd\n",
        "\n",
        "# Assume the predicted_texts is the output of your model.\n",
        "for i, predicted_text in enumerate(predicted_texts):\n",
        "    # Check if the predicted text exists in the text_data dictionary.\n",
        "    if predicted_text in text_data:\n",
        "        # Generate the reference speech using gTTS for the actual sentence.\n",
        "        actual_sentence = text_data[predicted_text]\n",
        "        tts_ref = gTTS(actual_sentence)\n",
        "        reference_file = f'reference_{i}.mp3'\n",
        "        tts_ref.save(reference_file)  # Save the reference speech as an MP3 file.\n",
        "\n",
        "        # Generate the speech from the predicted text using gTTS.\n",
        "        tts_pred = gTTS(predicted_text)\n",
        "        generated_file = f'generated_{i}.mp3'\n",
        "        tts_pred.save(generated_file)  # Save the generated speech as an MP3 file.\n",
        "\n",
        "        # Extract MFCC features from both the reference and generated speech.\n",
        "        mfcc_ref = extract_mfcc(reference_file)\n",
        "        mfcc_gen = extract_mfcc(generated_file)\n",
        "\n",
        "        # Calculate the Mel-Cepstral Distortion (MCD) between the MFCC features.\n",
        "        mcd_value = calculate_mcd(mfcc_gen, mfcc_ref)\n",
        "        print(f\"MCD for sentence {i}: {mcd_value:.4f}\")  # Output the MCD value for each sentence.\n",
        "    else:\n",
        "        # Handle cases where the predicted text does not match any reference text.\n",
        "        print(f\"Predicted text '{predicted_text}' not found in text_data. Skipping this sentence.\")\n"
      ],
      "metadata": {
        "id": "8uBx0dSc3L6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import euclidean\n",
        "\n",
        "# Function to calculate Signal-to-Noise Ratio (SNR)\n",
        "def calculate_snr(reference_file, generated_file, epsilon=1e-10):\n",
        "\n",
        "    ref_audio, sr_ref = librosa.load(reference_file, sr=None)\n",
        "    gen_audio, sr_gen = librosa.load(generated_file, sr=None)\n",
        "\n",
        "    # Ensure both files have the same length by truncating to the minimum length.\n",
        "    min_len = min(len(ref_audio), len(gen_audio))\n",
        "    ref_audio = ref_audio[:min_len]\n",
        "    gen_audio = gen_audio[:min_len]\n",
        "\n",
        "    # Calculate noise and add epsilon to avoid division by zero.\n",
        "    noise = ref_audio - gen_audio\n",
        "    noise_power = np.sum(noise ** 2) + epsilon  # Adding epsilon to avoid division by zero.\n",
        "    signal_power = np.sum(ref_audio ** 2)\n",
        "\n",
        "    # Calculate the Signal-to-Noise Ratio (SNR) in decibels.\n",
        "    snr = 10 * np.log10(signal_power / noise_power)\n",
        "    return snr\n",
        "\n",
        "# Function to calculate Mel-Cepstral Distortion (MCD)\n",
        "def calculate_mcd(mfcc1, mfcc2):\n",
        "\n",
        "    min_len = min(mfcc1.shape[1], mfcc2.shape[1])\n",
        "    mfcc1 = mfcc1[:, :min_len]  # Truncate MFCCs to the minimum length.\n",
        "    mfcc2 = mfcc2[:, :min_len]\n",
        "\n",
        "    # Calculate the average Euclidean distance between corresponding MFCC vectors.\n",
        "    mcd = np.mean([euclidean(mfcc1[:, i], mfcc2[:, i]) for i in range(min_len)])\n",
        "    return mcd\n",
        "\n",
        "# Function to extract MFCCs from an audio file\n",
        "def extract_mfcc(file_path, n_mfcc=13):\n",
        "\n",
        "    y, sr = librosa.load(file_path, sr=None)  # Load the audio file.\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)  # Extract MFCC features.\n",
        "    return mfcc\n",
        "\n",
        "# Lists to store SNR and MCD values for each sentence\n",
        "snr_values = []\n",
        "mcd_values = []\n",
        "\n",
        "# Iterate through the predicted texts and evaluate SNR and MCD\n",
        "for i, predicted_text in enumerate(predicted_texts):\n",
        "    if predicted_text in text_data:\n",
        "        # Generate the reference and generated audio files\n",
        "        actual_sentence = text_data[predicted_text]\n",
        "        tts_ref = gTTS(actual_sentence)\n",
        "        reference_file = f'reference_{i}.mp3'\n",
        "        tts_ref.save(reference_file)\n",
        "\n",
        "        tts_pred = gTTS(predicted_text)\n",
        "        generated_file = f'generated_{i}.mp3'\n",
        "        tts_pred.save(generated_file)\n",
        "\n",
        "        # Calculate SNR\n",
        "        snr_value = calculate_snr(reference_file, generated_file)\n",
        "        snr_values.append(snr_value)\n",
        "\n",
        "        # Extract MFCC features\n",
        "        mfcc_ref = extract_mfcc(reference_file)\n",
        "        mfcc_gen = extract_mfcc(generated_file)\n",
        "\n",
        "        # Calculate MCD\n",
        "        mcd_value = calculate_mcd(mfcc_gen, mfcc_ref)\n",
        "        mcd_values.append(mcd_value)\n",
        "    else:\n",
        "        # Handle cases where the predicted text does not match any reference text.\n",
        "        print(f\"Predicted text '{predicted_text}' not found in text_data. Skipping this sentence.\")\n",
        "\n",
        "# Summary statistics for SNR\n",
        "mean_snr = np.mean(snr_values)\n",
        "median_snr = np.median(snr_values)\n",
        "std_snr = np.std(snr_values)\n",
        "\n",
        "print(f\"Mean SNR: {mean_snr:.2f} dB\")\n",
        "print(f\"Median SNR: {median_snr:.2f} dB\")\n",
        "print(f\"Standard Deviation of SNR: {std_snr:.2f} dB\")\n",
        "\n",
        "# Summary statistics for MCD\n",
        "mean_mcd = np.mean(mcd_values)\n",
        "median_mcd = np.median(mcd_values)\n",
        "std_mcd = np.std(mcd_values)\n",
        "\n",
        "print(f\"Mean MCD: {mean_mcd:.4f}\")\n",
        "print(f\"Median MCD: {median_mcd:.4f}\")\n",
        "print(f\"Standard Deviation of MCD: {std_mcd:.4f}\")\n"
      ],
      "metadata": {
        "id": "rEQ0Qk4S3bFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualization of SNR\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(snr_values, bins=20, color='blue', edgecolor='black')\n",
        "plt.title('Histogram of SNR Values')\n",
        "plt.xlabel('SNR (dB)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.boxplot(snr_values, vert=False, patch_artist=True)\n",
        "plt.title('Boxplot of SNR Values')\n",
        "plt.xlabel('SNR (dB)')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualization of MCD\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(mcd_values, bins=20, color='green', edgecolor='black')\n",
        "plt.title('Histogram of MCD Values')\n",
        "plt.xlabel('MCD')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.boxplot(mcd_values, vert=False, patch_artist=True)\n",
        "plt.title('Boxplot of MCD Values')\n",
        "plt.xlabel('MCD')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "d7QlfVZj5jCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from gtts import gTTS\n",
        "import IPython.display as ipd\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the directory to save the audio files\n",
        "output_dir = '/content/drive/MyDrive/sign_language_dataset/generated_speech'\n",
        "os.makedirs(output_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
        "\n",
        "# Generate speech for the corrected texts and save them\n",
        "for idx, predicted_text in enumerate(predicted_texts):\n",
        "    # Create a file name for each audio\n",
        "    audio_file_path = os.path.join(output_dir, f'output_{idx}.mp3')\n",
        "\n",
        "    # Generate the speech audio\n",
        "    tts = gTTS(predicted_text)\n",
        "    tts.save(audio_file_path)\n",
        "\n",
        "    print(f\"Saved audio: {audio_file_path}\")\n",
        "    print(predicted_text)\n"
      ],
      "metadata": {
        "id": "e41u-Mux6Kwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spectrograms of Generated Speech"
      ],
      "metadata": {
        "id": "oCigHciS6VUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and generate spectrogram for each saved audio file\n",
        "for idx, predicted_text in enumerate(predicted_texts):\n",
        "    audio_file_path = os.path.join(output_dir, f'output_{idx}.mp3')\n",
        "\n",
        "    # Load the audio file\n",
        "    y, sr = librosa.load(audio_file_path, sr=None)\n",
        "\n",
        "    # Generate a mel-scaled spectrogram\n",
        "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
        "\n",
        "    # Convert to log scale (dB)\n",
        "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
        "\n",
        "    # Plot the spectrogram\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    librosa.display.specshow(S_dB, sr=sr, x_axis='time', y_axis='mel')\n",
        "    plt.colorbar(format='%+2.0f dB')\n",
        "    plt.title(f'Spectrogram for output_{idx}.mp3')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "fNZdRA-r6afe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the directory to save spectrogram images\n",
        "spectrogram_dir = '/content/drive/MyDrive/sign_language_dataset/generated_speech/spectrogram_images'\n",
        "os.makedirs(spectrogram_dir, exist_ok=True)\n",
        "\n",
        "# Load and generate spectrogram for each saved audio file\n",
        "for idx, predicted_text in enumerate(predicted_texts):\n",
        "    audio_file_path = os.path.join(output_dir, f'output_{idx}.mp3')\n",
        "\n",
        "    # Load the audio file\n",
        "    y, sr = librosa.load(audio_file_path, sr=None)\n",
        "\n",
        "    # Generate a mel-scaled spectrogram\n",
        "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
        "\n",
        "    # Convert to log scale (dB)\n",
        "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
        "\n",
        "    # Plot the spectrogram\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    librosa.display.specshow(S_dB, sr=sr, x_axis='time', y_axis='mel')\n",
        "    plt.colorbar(format='%+2.0f dB')\n",
        "    plt.title(f'Spectrogram for output_{idx}.mp3')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save the spectrogram image\n",
        "    spectrogram_file_path = os.path.join(spectrogram_dir, f'spectrogram_{idx}.png')\n",
        "    plt.savefig(spectrogram_file_path)\n",
        "    plt.close()  # Close the plot to free memory\n",
        "\n",
        "    print(f\"Spectrogram saved: {spectrogram_file_path}\")\n"
      ],
      "metadata": {
        "id": "Iqf2Mo1i6ejm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the directory to save spectrogram images\n",
        "spectrogram_dir = '/content/drive/MyDrive/sign_language_dataset/generated_speech/spectrogram_images'\n",
        "os.makedirs(spectrogram_dir, exist_ok=True)\n",
        "\n",
        "# Maximum number of subplots per figure\n",
        "max_subplots = 5\n",
        "\n",
        "# Process audio files and generate spectrograms\n",
        "for i in range(0, len(predicted_texts), max_subplots):\n",
        "    num_subplots = min(max_subplots, len(predicted_texts) - i)\n",
        "    fig, axs = plt.subplots(num_subplots, 1, figsize=(10, 4 * num_subplots))\n",
        "\n",
        "    # Ensure axs is iterable by making it a list if there's only one subplot\n",
        "    if num_subplots == 1:\n",
        "        axs = [axs]\n",
        "\n",
        "    for j in range(num_subplots):\n",
        "        idx = i + j\n",
        "        audio_file_path = os.path.join(output_dir, f'output_{idx}.mp3')\n",
        "\n",
        "        # Load the audio file\n",
        "        y, sr = librosa.load(audio_file_path, sr=None)\n",
        "\n",
        "        # Generate a mel-scaled spectrogram\n",
        "        S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
        "        S_dB = librosa.power_to_db(S, ref=np.max)\n",
        "\n",
        "        # Plot on the respective subplot\n",
        "        img = librosa.display.specshow(S_dB, sr=sr, x_axis='time', y_axis='mel', ax=axs[j])\n",
        "        axs[j].set_title(f'Spectrogram of the generated speech')\n",
        "\n",
        "    # Add a colorbar for the last spectrogram in the figure\n",
        "    fig.colorbar(img, ax=axs, format='%+2.0f dB')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    plt.close(fig)  # Close the figure to free memory\n"
      ],
      "metadata": {
        "id": "ANtc6Oaa6lBz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}